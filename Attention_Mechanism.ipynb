{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6gL1zcorKwE"
      },
      "source": [
        "# **Self Attention Mechanism**\n",
        "*`The self-attention mechanism is a key component of Transformers, enabling each element in an input sequence to attend to all other elements. This generates a weighted representation of the input, allowing the model to capture dependencies between tokens, regardless of their positions.`*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maX9-hYj5qg5"
      },
      "source": [
        "`Before starting with Self Attention Mechanism you should have the basics of Embeddings.`\\\n",
        "**Input Representation:**\\\n",
        "Each token in the input sequence `(e.g., a word or subword)` is converted into a vector `(embedding)` that represents its meaning in context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V42Lm15G8AA-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhbSKuax5Q6B"
      },
      "source": [
        "### **Defining the Weight Matrices**\n",
        "`supposing embeddings is defined and it has a shape of (10,16) after reshape. (initially (1,10,16))`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tap4gbu6kiW"
      },
      "source": [
        "Create Queries, Keys, and Values: Transform each embedding into three different vectors: `Query (Q)`, `Key (K)`, and `Value (V)` using learned weight matrices.\\\n",
        "**Query (Q):** Represents \"what this token wants to focus on.\"\\\n",
        "**Key (K):** Represents \"the tokens being compared to.\"\\\n",
        "**Value (V):** Represents \"the actual information this token carries.\"\\\n",
        "\n",
        "**Shape of Q, K, V should be** `(batch_size, seq_length, attention_dim)`\\\n",
        "\n",
        "\n",
        "> In this example batch_size is 1. we only have 1 sequence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbkxkTI_K00l",
        "outputId": "9ef1c049-a932-414d-c936-223a944f4bf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------\n",
            "embed_size: 16\n",
            "---------------------------\n",
            "w_q Shape : (16, 16)\n",
            "---------------------------\n",
            "w_k Shape : (16, 16)\n",
            "---------------------------\n",
            "w_v Shape : (16, 16)\n",
            "---------------------------\n"
          ]
        }
      ],
      "source": [
        "embed_dim = embeddings.shape[1] # embeddings dim\n",
        "attention_dim = embed_dim // 1 # 1 head\n",
        "\n",
        "w_q = np.random.randn(attention_dim, embed_dim)\n",
        "w_k = np.random.randn(attention_dim, embed_dim)\n",
        "w_v = np.random.randn(attention_dim, embed_dim)\n",
        "\n",
        "print(\"---------------------------\")\n",
        "print(f\"embed_size: {embed_dim}\")\n",
        "print(\"---------------------------\")\n",
        "print(f\"w_q Shape : {w_q.shape}\")\n",
        "print(\"---------------------------\")\n",
        "print(f\"w_k Shape : {w_k.shape}\")\n",
        "print(\"---------------------------\")\n",
        "print(f\"w_v Shape : {w_v.shape}\")\n",
        "print(\"---------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Whu_rodY7Emu"
      },
      "source": [
        "Each input embedding ð‘‹ (for each token) is multiplied by \"learned\" weight matrices **(w_q, w_k, w_v: `Shape = (attention_dim, embed_dim)`)**\\\n",
        "\n",
        "\n",
        "> attention_dim = embed_dim // numberOfHeads\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Giy9_AX52yT2",
        "outputId": "c0677a85-db2a-40d7-deea-d1a16dfd4744"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------\n",
            "Q Shape : (10, 16)\n",
            "-----------------------------\n",
            "K Shape : (10, 16)\n",
            "-----------------------------\n",
            "V Shape : (10, 16)\n",
            "-----------------------------\n"
          ]
        }
      ],
      "source": [
        "# Calculate \"queries (Q)\", \"keys (K)\", and \"values (V)\" matrices\n",
        "queries = np.matmul(embeddings, w_q.T)\n",
        "keys = np.matmul(embeddings, w_k.T)\n",
        "values = np.matmul(embeddings, w_v.T)\n",
        "\n",
        "print(\"-----------------------------\")\n",
        "print(f\"Q Shape : {queries.shape}\")\n",
        "print(\"-----------------------------\")\n",
        "print(f\"K Shape : {keys.shape}\")\n",
        "print(\"-----------------------------\")\n",
        "print(f\"V Shape : {values.shape}\")\n",
        "print(\"-----------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cTibCLpArym"
      },
      "source": [
        "### **Compute Attention Scores**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAWtJlIb6Als",
        "outputId": "9ad009c7-c89f-4ac6-e3f1-8c05cfba8237"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention Scores:\n",
            " [[ 2.70287240e-03  2.70287240e-03  1.78910538e-02  9.55023619e-03\n",
            "   1.88574373e-02 -1.62622976e-02 -1.98783640e-02  4.45656019e-04\n",
            "  -3.13456869e-03 -4.76539751e-03]\n",
            " [ 2.70287240e-03  2.70287240e-03  1.78910538e-02  9.55023619e-03\n",
            "   1.88574373e-02 -1.62622976e-02 -1.98783640e-02  4.45656019e-04\n",
            "  -3.13456869e-03 -4.76539751e-03]\n",
            " [-8.57778044e-03 -8.57778044e-03  2.13849131e-02 -2.00194034e-02\n",
            "   2.57808482e-02 -1.66025541e-03 -1.47558982e-02 -6.66359595e-03\n",
            "   7.32537604e-03  4.55469433e-03]\n",
            " [ 2.69862993e-02  2.69862993e-02 -3.60754410e-03 -2.43195157e-03\n",
            "  -1.86257856e-03 -3.38294556e-03  5.16355997e-03 -5.76117284e-03\n",
            "   2.22295662e-03 -7.59801258e-03]\n",
            " [-1.53231871e-02 -1.53231871e-02  1.66089440e-02 -1.81660526e-02\n",
            "   1.90844939e-02 -5.67780452e-03 -1.52599985e-02 -1.19689781e-02\n",
            "   1.54026757e-02  1.07075743e-02]\n",
            " [-5.40705112e-03 -5.40705112e-03  6.90545702e-03 -9.45911361e-03\n",
            "   1.62412810e-02  3.04873908e-04 -1.08263818e-02 -6.13013830e-03\n",
            "   1.12018576e-02  3.67360809e-03]\n",
            " [-2.23403525e-02 -2.23403525e-02  1.32462734e-02 -1.09223143e-02\n",
            "   2.08348852e-02 -7.85651587e-03 -6.43803174e-03  4.96769029e-03\n",
            "   6.00513121e-03 -9.64946223e-03]\n",
            " [ 1.07325438e-02  1.07325438e-02  2.81930338e-02 -9.49311133e-03\n",
            "   2.20053441e-02 -5.21763470e-03 -1.92476621e-02  8.03609964e-03\n",
            "  -1.81889465e-03 -2.18861489e-02]\n",
            " [-1.05838519e-02 -1.05838519e-02  4.05279370e-03  3.13961279e-03\n",
            "  -1.13224321e-02 -6.62859640e-03 -2.46249824e-03 -1.32174415e-04\n",
            "   3.14407221e-03 -1.76855421e-02]\n",
            " [ 1.10286429e-02  1.10286429e-02  1.22594238e-02  1.48236053e-02\n",
            "  -4.76672473e-03 -9.77695844e-03 -4.04724164e-03  1.71116498e-02\n",
            "  -7.65462324e-05 -2.32818795e-02]] \n",
            " Shape : (10, 10)\n"
          ]
        }
      ],
      "source": [
        "# Calculate attention scores\n",
        "attention_scores = np.matmul(queries, keys.T)\n",
        "# scale the scores\n",
        "attention_scores = attention_scores / np.sqrt(attention_dim)\n",
        "print(f\"Attention Scores:\\n {attention_scores} \\n Shape : {attention_scores.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yp4W8BhA0Ca"
      },
      "source": [
        "### **Compute attention weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byxBCuxg7Bko",
        "outputId": "9091da76-cdd6-44b1-b894-4b410951decd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention Weights:\n",
            " [[0.10018203 0.10018203 0.10171522 0.10087036 0.10181357 0.09829996\n",
            "  0.09794514 0.09995615 0.09959892 0.09943663]\n",
            " [0.10018203 0.10018203 0.10171522 0.10087036 0.10181357 0.09829996\n",
            "  0.09794514 0.09995615 0.09959892 0.09943663]\n",
            " [0.09914789 0.09914789 0.10216358 0.09801994 0.10261368 0.09983613\n",
            "  0.09853723 0.09933786 0.10073726 0.10045854]\n",
            " [0.10235127 0.10235127 0.09926737 0.09938413 0.09944074 0.09928966\n",
            "  0.10014188 0.09905381 0.09984784 0.09887203]\n",
            " [0.09866501 0.09866501 0.10186643 0.09838492 0.10211892 0.09962127\n",
            "  0.09867124 0.09899651 0.10174363 0.10126705]\n",
            " [0.09944609 0.09944609 0.10067809 0.09904394 0.1016224  0.10001574\n",
            "  0.09890862 0.09937421 0.10111157 0.10035324]\n",
            " [0.09811947 0.09811947 0.10167408 0.09924622 0.10244858 0.09955095\n",
            "  0.09969227 0.10083584 0.1009405  0.09937262]\n",
            " [0.10084396 0.10084396 0.1026202  0.09882481 0.10198718 0.09924824\n",
            "  0.0978655  0.1005724  0.09958613 0.09760763]\n",
            " [0.09943131 0.09943131 0.10089736 0.10080526 0.0993579  0.09982537\n",
            "  0.10024212 0.10047599 0.10080571 0.09872768]\n",
            " [0.10085588 0.10085588 0.10098009 0.10123935 0.09927534 0.09877919\n",
            "  0.09934679 0.10147126 0.09974205 0.09745415]] \n",
            " Shape : (10, 10)\n"
          ]
        }
      ],
      "source": [
        "attention_weights = tf.nn.softmax(attention_scores, axis=-1)\n",
        "print(f\"Attention Weights:\\n {attention_weights} \\n Shape : {attention_weights.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJTemaSYBAKi"
      },
      "source": [
        "### **Compute weighted sum**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6IxzX307H5Y",
        "outputId": "52a64980-9d51-4b96-a966-2ba123eb4836"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weighted Sum:\n",
            " [[-0.07191973  0.05369578  0.01552018 -0.01301887 -0.06879538  0.05129807\n",
            "   0.02357675 -0.03675518  0.10005823  0.06798076  0.06032971  0.03579166\n",
            "   0.00610018 -0.02639282  0.06365587 -0.05795981]\n",
            " [-0.07191973  0.05369578  0.01552018 -0.01301887 -0.06879538  0.05129807\n",
            "   0.02357675 -0.03675518  0.10005823  0.06798076  0.06032971  0.03579166\n",
            "   0.00610018 -0.02639282  0.06365587 -0.05795981]\n",
            " [-0.0712428   0.05377908  0.01586796 -0.01391785 -0.06824096  0.05154462\n",
            "   0.02331869 -0.03595441  0.09951587  0.06805199  0.06093533  0.03547583\n",
            "   0.00571188 -0.02641789  0.06426359 -0.05854774]\n",
            " [-0.07198051  0.05478058  0.01604369 -0.01293537 -0.06847798  0.04998116\n",
            "   0.02426112 -0.03690346  0.10076066  0.06731393  0.05979927  0.03576031\n",
            "   0.00597596 -0.02498828  0.06373311 -0.05658847]\n",
            " [-0.07147503  0.05334159  0.0157559  -0.01353773 -0.06797707  0.0516849\n",
            "   0.02328957 -0.03570395  0.09971888  0.06814356  0.06105552  0.03568642\n",
            "   0.00592407 -0.02674121  0.06426525 -0.05844505]\n",
            " [-0.07166933  0.0535817   0.01578636 -0.01322063 -0.0680265   0.05133026\n",
            "   0.02348553 -0.03597171  0.10003138  0.06806059  0.06064781  0.03576087\n",
            "   0.00591133 -0.02627888  0.06411676 -0.05798704]\n",
            " [-0.07129637  0.05309509  0.01579672 -0.01352936 -0.06793638  0.0516212\n",
            "   0.02320564 -0.03585152  0.0998098   0.0684776   0.0606036   0.0355561\n",
            "   0.00581903 -0.02657091  0.06398521 -0.05828759]\n",
            " [-0.07128811  0.05441211  0.01594661 -0.01419342 -0.06894397  0.05083747\n",
            "   0.0239941  -0.03710803  0.09965663  0.06792892  0.06042168  0.03517632\n",
            "   0.00566631 -0.02591925  0.06370307 -0.05839113]\n",
            " [-0.0719517   0.05315304  0.01584329 -0.01266356 -0.06779614  0.05080023\n",
            "   0.02371268 -0.03626432  0.10071756  0.06827684  0.06001612  0.0359131\n",
            "   0.00619092 -0.02592554  0.06362539 -0.05716033]\n",
            " [-0.0720617   0.0537627   0.01589878 -0.01265539 -0.06854331  0.05045671\n",
            "   0.02419454 -0.03722562  0.10096424  0.06812864  0.0596662   0.03574647\n",
            "   0.00619372 -0.02575486  0.06344401 -0.05708099]] \n",
            " Shape : (10, 16)\n"
          ]
        }
      ],
      "source": [
        "weighted_sum = np.matmul(attention_weights, values)\n",
        "print(f\"Weighted Sum:\\n {weighted_sum} \\n Shape : {weighted_sum.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sOXHAaI7P1B",
        "outputId": "cc005a0d-7da8-4408-c0e5-6e6d6de6e593"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-0.07191973,  0.05369578,  0.01552018, -0.01301887, -0.06879538,\n",
              "        0.05129807,  0.02357675, -0.03675518,  0.10005823,  0.06798076,\n",
              "        0.06032971,  0.03579166,  0.00610018, -0.02639282,  0.06365587,\n",
              "       -0.05795981])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the weighted sum of the second token\n",
        "weighted_sum[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dOTiB7X8GE2"
      },
      "source": [
        "# **Multi-Head Attention Mechanism**\n",
        "*`In multi-head attention in Transformers, there are multiple attention heads, each with its own set of query, key, and value matrices. Each head focuses on different parts of the input data and learns different relationships between words (or tokens) in the sequence. By doing so, the model can capture different aspects of the sequence simultaneously.`*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkEmPMdS9OOh",
        "outputId": "c6904aab-2f81-4475-cec4-dd764b2d91ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10, 16)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check embeddings shape\n",
        "embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6n_1len48E7e"
      },
      "outputs": [],
      "source": [
        "h = 3 # number of attention heads\n",
        "\n",
        "# Initialize the weights\n",
        "multiHead_W_q, multiHead_W_k, multiHead_W_v = np.random.randn(h, attention_dim, embed_dim), np.random.randn(h, attention_dim, embed_dim), np.random.randn(h, attention_dim, embed_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LF2_26cA8gmL",
        "outputId": "bf97cac5-6e9a-4a7a-8efa-1646cf2880f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "Queries:\n",
            " [[[ 0.18487642 -0.07525517  0.01157229  0.04350997 -0.0645356\n",
            "    0.21331945  0.08393987 -0.12669617 -0.13951258  0.00223234\n",
            "   -0.05053065 -0.09170585 -0.05476004 -0.06682444 -0.15111652\n",
            "    0.02846103]\n",
            "  [ 0.18487642 -0.07525517  0.01157229  0.04350997 -0.0645356\n",
            "    0.21331945  0.08393987 -0.12669617 -0.13951258  0.00223234\n",
            "   -0.05053065 -0.09170585 -0.05476004 -0.06682444 -0.15111652\n",
            "    0.02846103]\n",
            "  [-0.11771118  0.0400669  -0.04372065 -0.10702574  0.07184724\n",
            "    0.04635327 -0.01170662 -0.07947526  0.07866789 -0.11991283\n",
            "    0.05006804  0.06260486 -0.20345729 -0.06922961 -0.04266434\n",
            "   -0.02847235]\n",
            "  [ 0.08979838 -0.04733125  0.18040464  0.1979478  -0.18076144\n",
            "    0.00212834  0.05924504  0.42180878 -0.07092024 -0.01344724\n",
            "    0.02770039  0.14239369  0.08854688  0.13258597  0.0444258\n",
            "   -0.11311032]\n",
            "  [-0.19124372  0.16710342 -0.02560563 -0.23831425  0.11031772\n",
            "   -0.03766105  0.04390058 -0.08066733  0.2568171  -0.13622504\n",
            "    0.04085489  0.08333729 -0.14221918 -0.09981615 -0.09943792\n",
            "    0.1174098 ]\n",
            "  [-0.20323496  0.04035311  0.14572513 -0.1398236  -0.11648535\n",
            "   -0.07283699 -0.16902471 -0.05096883  0.01536746 -0.05570993\n",
            "    0.11977066 -0.01357955  0.0989877   0.12156611 -0.00204827\n",
            "   -0.11174273]\n",
            "  [-0.16603984 -0.12126918 -0.03258415 -0.0553519  -0.03499233\n",
            "   -0.08503139 -0.07143611 -0.01881048 -0.11744858 -0.03732191\n",
            "    0.14151961  0.01016813  0.00884234  0.18143021  0.08781238\n",
            "   -0.11070603]\n",
            "  [ 0.02409449  0.05274939  0.00667843 -0.06172713 -0.13041757\n",
            "   -0.06566919 -0.13158973 -0.21225304 -0.07271289 -0.02734668\n",
            "    0.10113629 -0.0868016  -0.11513206 -0.03894837 -0.10689924\n",
            "   -0.24791397]\n",
            "  [ 0.00220849 -0.00984776 -0.06451397  0.11842077 -0.06274098\n",
            "    0.01684555 -0.06936533 -0.01656866  0.12082044 -0.07034435\n",
            "   -0.01473955  0.03900867  0.00895104 -0.22503789 -0.00903053\n",
            "   -0.13771588]\n",
            "  [ 0.10345375  0.01721243  0.07913529  0.04656189 -0.37680055\n",
            "   -0.12481831 -0.02446958 -0.21547973 -0.01916316  0.13062371\n",
            "   -0.19564306 -0.22584772  0.32222564  0.01596211 -0.18273265\n",
            "   -0.24484469]]\n",
            "\n",
            " [[-0.06792917 -0.11792272  0.02645035  0.09051076  0.22920833\n",
            "    0.04953084 -0.01075066 -0.09072305  0.04950198 -0.02951349\n",
            "   -0.0247308  -0.08054511 -0.13868614 -0.17353772 -0.00451069\n",
            "    0.1774648 ]\n",
            "  [-0.06792917 -0.11792272  0.02645035  0.09051076  0.22920833\n",
            "    0.04953084 -0.01075066 -0.09072305  0.04950198 -0.02951349\n",
            "   -0.0247308  -0.08054511 -0.13868614 -0.17353772 -0.00451069\n",
            "    0.1774648 ]\n",
            "  [ 0.10252417  0.05649338  0.04689705  0.16333224 -0.16543907\n",
            "    0.04741486 -0.07138233 -0.13457062 -0.10292155 -0.1712104\n",
            "    0.02949471  0.05349869 -0.08966171 -0.2403688   0.10864777\n",
            "   -0.06751534]\n",
            "  [ 0.03285489  0.24047202 -0.08984254  0.0020806   0.05291461\n",
            "    0.15879634  0.09388782  0.09650328  0.12374147  0.1151249\n",
            "   -0.06843795 -0.0956063   0.17282433 -0.07156195  0.15195793\n",
            "    0.16304563]\n",
            "  [ 0.04304545  0.02985701  0.10987549  0.27610195 -0.21912613\n",
            "    0.11179922 -0.00843482 -0.07795517 -0.02749783 -0.09418357\n",
            "    0.00197751  0.07294756 -0.06252434 -0.29787964  0.04960142\n",
            "    0.00626273]\n",
            "  [-0.11044847 -0.07978791 -0.09018289  0.11634044  0.08542153\n",
            "   -0.03660476 -0.16728896  0.07724269 -0.02680525  0.07390885\n",
            "   -0.14844807 -0.09062193  0.04447195  0.00662353  0.10883752\n",
            "    0.02050721]\n",
            "  [-0.10701049 -0.17650533 -0.16603881 -0.14351348 -0.01034415\n",
            "   -0.11309924 -0.13279704 -0.01374367  0.03081129  0.04802551\n",
            "   -0.06253369  0.06278393  0.04208668  0.0435159  -0.07209562\n",
            "   -0.02726698]\n",
            "  [ 0.16327553 -0.00399893  0.12006611  0.0111471  -0.02614431\n",
            "   -0.14720959 -0.04043654 -0.22940293 -0.34458701 -0.0609352\n",
            "    0.02252211 -0.07079737 -0.11199091 -0.07568832  0.03773725\n",
            "   -0.21334292]\n",
            "  [-0.07072646  0.01947557 -0.06433109  0.04100813 -0.21599761\n",
            "    0.00311088 -0.02339117 -0.12145451  0.07800468  0.01942461\n",
            "    0.00903836  0.02483481  0.0181995   0.03116963 -0.03373549\n",
            "   -0.02645647]\n",
            "  [-0.16954805 -0.07761959 -0.12556741  0.02336329  0.13947915\n",
            "    0.02994787 -0.08384141  0.05102228 -0.23313719  0.04977657\n",
            "   -0.22731459 -0.26724241  0.0004822  -0.00424112 -0.04822002\n",
            "    0.07775123]]\n",
            "\n",
            " [[-0.01546829  0.06005626  0.04733779  0.09934191 -0.0831899\n",
            "    0.10050675 -0.06051309  0.10935537 -0.04890099  0.07702098\n",
            "   -0.18651268  0.05221248 -0.06934985 -0.20925267  0.08236633\n",
            "    0.11336222]\n",
            "  [-0.01546829  0.06005626  0.04733779  0.09934191 -0.0831899\n",
            "    0.10050675 -0.06051309  0.10935537 -0.04890099  0.07702098\n",
            "   -0.18651268  0.05221248 -0.06934985 -0.20925267  0.08236633\n",
            "    0.11336222]\n",
            "  [ 0.05266233  0.01359986  0.09404458 -0.06461014  0.11061109\n",
            "   -0.02342166 -0.12062478  0.07597054 -0.10560775  0.18876884\n",
            "    0.02361868 -0.03393905 -0.07822154 -0.03719087 -0.05087751\n",
            "   -0.12036049]\n",
            "  [-0.03567292  0.11998259 -0.02954549 -0.00919073 -0.04513234\n",
            "    0.03131584  0.21888636  0.05260012  0.17689176 -0.18138357\n",
            "    0.24670423  0.13107742  0.16352027  0.02959081 -0.05090617\n",
            "    0.05115904]\n",
            "  [ 0.24938433  0.08361238  0.05629585 -0.03125635  0.01111642\n",
            "   -0.08215068 -0.27472939  0.10742293 -0.05774561  0.33657862\n",
            "    0.07773722 -0.07001442 -0.12695316 -0.16638083  0.13620662\n",
            "   -0.06816156]\n",
            "  [ 0.09917422 -0.03019577 -0.02995996 -0.08891208  0.00826485\n",
            "   -0.02895133 -0.08211951 -0.1378336  -0.0210061   0.1230229\n",
            "    0.06433484  0.05492987 -0.0054026  -0.0157969  -0.10794357\n",
            "    0.09606416]\n",
            "  [ 0.01675346 -0.04319675 -0.09998382 -0.05790359  0.23079117\n",
            "   -0.0904684  -0.09033459 -0.08859938  0.01211631 -0.01559433\n",
            "   -0.10128715  0.06836005  0.1048924  -0.03992851  0.01199205\n",
            "   -0.02720579]\n",
            "  [ 0.09957205 -0.09305551  0.15962867 -0.09329644 -0.00320906\n",
            "    0.14137824  0.05448031 -0.02034055 -0.10663277 -0.00058194\n",
            "    0.1598759   0.19658304  0.04292724  0.00885857 -0.25700596\n",
            "   -0.18527471]\n",
            "  [-0.1575424  -0.06933363  0.04835835 -0.02662093 -0.07826078\n",
            "   -0.04587659 -0.05784396 -0.04047856  0.02055902  0.03539149\n",
            "    0.17764713 -0.01759025 -0.04282837  0.08183561 -0.1821469\n",
            "    0.14444058]\n",
            "  [ 0.16764852  0.01598308  0.12994675  0.05856881 -0.27672271\n",
            "    0.06250078 -0.04315688 -0.09574273  0.19555601  0.02650009\n",
            "    0.14701018  0.05846728  0.14966796  0.03614775  0.02916283\n",
            "    0.18410558]]] \n",
            " Shape : (3, 10, 16)\n",
            "----------------------------------------\n",
            "Keys:\n",
            " [[[ 0.09092988 -0.08104183  0.041375   -0.09058609  0.09686783\n",
            "   -0.25421444  0.09314345 -0.22819697  0.05736043  0.12947282\n",
            "    0.0164126   0.05363871  0.19404439  0.07021956 -0.02523634\n",
            "    0.02731462]\n",
            "  [ 0.09092988 -0.08104183  0.041375   -0.09058609  0.09686783\n",
            "   -0.25421444  0.09314345 -0.22819697  0.05736043  0.12947282\n",
            "    0.0164126   0.05363871  0.19404439  0.07021956 -0.02523634\n",
            "    0.02731462]\n",
            "  [ 0.05532255  0.02601678  0.03624988  0.02104857  0.08203039\n",
            "    0.08528507  0.10973486 -0.06062211 -0.15462605  0.03116244\n",
            "    0.01124673  0.0449006  -0.02840784 -0.0245865   0.09618249\n",
            "    0.11232894]\n",
            "  [ 0.16790143 -0.12548028 -0.22265975  0.18259728 -0.29445143\n",
            "    0.15024024  0.04908758  0.14041095  0.29605226 -0.01644942\n",
            "    0.01318183 -0.04432818  0.13427945 -0.01938779  0.14643139\n",
            "   -0.19077127]\n",
            "  [ 0.06905143  0.09840638  0.09280366  0.00438938  0.14063926\n",
            "    0.02282492 -0.01634274 -0.04533508 -0.1141061  -0.00313904\n",
            "   -0.00860609 -0.02557869 -0.03582164  0.1113915   0.08998394\n",
            "    0.23903944]\n",
            "  [-0.10822285 -0.03284927 -0.05873722 -0.0736999  -0.02561444\n",
            "    0.05206224 -0.1602218  -0.21168957  0.00885501 -0.0312848\n",
            "    0.05921829 -0.13234834 -0.19531328  0.05100851 -0.04675267\n",
            "    0.04438416]\n",
            "  [-0.17173332 -0.14301648  0.00141772 -0.07668856  0.03879002\n",
            "   -0.0614243  -0.13157719 -0.26603724  0.08387347  0.0451578\n",
            "   -0.05781553 -0.01735047 -0.14055047  0.09026613 -0.2364429\n",
            "    0.06869166]\n",
            "  [-0.16475102 -0.16269761 -0.03054413  0.07021706 -0.04710457\n",
            "    0.09561432  0.09183588  0.01234321 -0.1234761  -0.05382213\n",
            "    0.02296469  0.07812328  0.08164217  0.01839588  0.11332807\n",
            "    0.01312662]\n",
            "  [ 0.20156939 -0.12394535  0.06532477  0.05273772  0.12412817\n",
            "   -0.039683    0.21450242  0.08923368 -0.04872081 -0.03605378\n",
            "   -0.02880537  0.09475953 -0.00870453 -0.08454378  0.18164603\n",
            "    0.04961941]\n",
            "  [-0.08654126 -0.24967418 -0.10700059 -0.10501163 -0.06139872\n",
            "   -0.22100056 -0.09125032  0.02638212  0.01387193  0.01862853\n",
            "    0.08561644  0.01384764  0.05907459  0.19331504  0.02986145\n",
            "    0.16737851]]\n",
            "\n",
            " [[ 0.07962591  0.08763401 -0.17170591 -0.12008729  0.08028078\n",
            "   -0.18826485  0.06268426 -0.0642804  -0.3194644   0.01708014\n",
            "    0.07328658  0.06195404 -0.09211745  0.04360871  0.31176463\n",
            "   -0.09583388]\n",
            "  [ 0.07962591  0.08763401 -0.17170591 -0.12008729  0.08028078\n",
            "   -0.18826485  0.06268426 -0.0642804  -0.3194644   0.01708014\n",
            "    0.07328658  0.06195404 -0.09211745  0.04360871  0.31176463\n",
            "   -0.09583388]\n",
            "  [ 0.12547889 -0.24752795  0.03828678  0.16609518  0.07332319\n",
            "    0.04832957 -0.03201889  0.13325043 -0.11412449  0.01912006\n",
            "    0.00308298  0.14894714  0.03141323  0.125566   -0.0752998\n",
            "   -0.08807698]\n",
            "  [-0.2282884   0.00050091  0.03243947  0.072682    0.15331254\n",
            "   -0.06215658  0.06371361 -0.16595425  0.07355572 -0.0708928\n",
            "    0.1964661  -0.14478524  0.11904402 -0.05859548  0.15579376\n",
            "    0.09719615]\n",
            "  [ 0.00915846 -0.27302347 -0.03202637  0.12612979  0.06884848\n",
            "    0.02210919 -0.11590369  0.08248757 -0.32332923  0.10026622\n",
            "   -0.06185912  0.06439897 -0.00751686 -0.02187362 -0.08104034\n",
            "   -0.03562014]\n",
            "  [ 0.07131001 -0.13271215  0.01545254  0.07589355 -0.08321891\n",
            "   -0.05813484 -0.00163513  0.1178461   0.07792524 -0.14264734\n",
            "   -0.03097117 -0.14281481  0.02856661  0.07539921 -0.04045053\n",
            "   -0.06278683]\n",
            "  [-0.01406073  0.06651761 -0.0036035  -0.0508168  -0.11035768\n",
            "   -0.00643041  0.0259112  -0.07059317  0.11412973 -0.09849615\n",
            "   -0.04214381 -0.18331189 -0.12732832 -0.00039246 -0.02020515\n",
            "    0.01124758]\n",
            "  [ 0.17908853 -0.14555741  0.02247861  0.09036658  0.0557462\n",
            "    0.1120988   0.06414938 -0.00115541  0.23584158 -0.05680553\n",
            "   -0.12936754  0.19031837  0.07129747  0.16120531 -0.05290245\n",
            "   -0.03961386]\n",
            "  [ 0.12321697 -0.06797911  0.0510489  -0.02355909 -0.08415416\n",
            "    0.00355728  0.01477874  0.28296749 -0.0105167  -0.06316455\n",
            "    0.06036487  0.19846574  0.07574273  0.10082822 -0.11202982\n",
            "   -0.1540349 ]\n",
            "  [ 0.01636879  0.01934824 -0.11943825 -0.23935557  0.0933673\n",
            "   -0.041061   -0.02126829 -0.01814621 -0.1131223   0.01255391\n",
            "    0.09642704  0.07401361  0.11109733 -0.03193849  0.02825417\n",
            "   -0.1364878 ]]\n",
            "\n",
            " [[ 0.0753647  -0.19386708  0.08555562 -0.29090231 -0.15108735\n",
            "   -0.09787369 -0.06291082 -0.01095187 -0.04746239 -0.01665215\n",
            "    0.03509109  0.11481996 -0.02639402 -0.1996855   0.1759072\n",
            "   -0.0366919 ]\n",
            "  [ 0.0753647  -0.19386708  0.08555562 -0.29090231 -0.15108735\n",
            "   -0.09787369 -0.06291082 -0.01095187 -0.04746239 -0.01665215\n",
            "    0.03509109  0.11481996 -0.02639402 -0.1996855   0.1759072\n",
            "   -0.0366919 ]\n",
            "  [ 0.00312552 -0.07180992  0.04043789  0.05403119  0.22941929\n",
            "   -0.06577426  0.03475648 -0.02561773  0.04320612  0.09040951\n",
            "   -0.12233266  0.05432803  0.08174871  0.04036027 -0.09201536\n",
            "   -0.11935073]\n",
            "  [ 0.04142688  0.09573231  0.02230457 -0.063899   -0.18153818\n",
            "    0.03852445 -0.1813879  -0.132454   -0.3286167   0.02421521\n",
            "    0.04758258 -0.13598693 -0.04607895  0.16237249 -0.0728456\n",
            "   -0.0935991 ]\n",
            "  [-0.01882842 -0.06536332 -0.09558912  0.08629794  0.25693593\n",
            "   -0.10479196 -0.00264568 -0.04505107  0.16211366  0.10881605\n",
            "   -0.22236821  0.099534    0.09899327 -0.09520909 -0.10183356\n",
            "   -0.16607477]\n",
            "  [-0.01864705 -0.00848388  0.00748007  0.02951311  0.02078259\n",
            "   -0.06858888  0.11523432 -0.19759504 -0.06614436  0.18976108\n",
            "    0.02892677 -0.06495285 -0.21756851 -0.05260464  0.05426891\n",
            "   -0.01840577]\n",
            "  [-0.01014297  0.03542493 -0.00335034  0.10020632  0.06908388\n",
            "   -0.01243841  0.05410721  0.08422439 -0.03600613  0.14762913\n",
            "    0.18702278 -0.13275551 -0.15795226 -0.06183359  0.14927207\n",
            "    0.04520779]\n",
            "  [ 0.1369354  -0.27082906  0.21420782 -0.07597131  0.18659728\n",
            "   -0.06551299  0.10181537  0.01644863 -0.06770142  0.02713303\n",
            "    0.03921735  0.05708617  0.141701   -0.06291641 -0.01605914\n",
            "    0.0722429 ]\n",
            "  [-0.00567622  0.0578198   0.10812209 -0.01266212 -0.03092867\n",
            "    0.06130452  0.1602579  -0.1543575   0.11775726  0.01412681\n",
            "   -0.08900508  0.16298173  0.05283902  0.04291816  0.02482643\n",
            "   -0.01511942]\n",
            "  [ 0.1042853  -0.22990015  0.01451894 -0.28993885 -0.02708887\n",
            "   -0.12855235 -0.06531142 -0.17209656 -0.01141655 -0.05680447\n",
            "    0.00376966  0.20394948  0.03690959 -0.34713192 -0.00552357\n",
            "    0.1118358 ]]] \n",
            " Shape : (3, 10, 16)\n",
            "----------------------------------------\n",
            "Values:\n",
            " [[[-1.41264938e-01 -1.13608662e-01  1.08270592e-01 -1.36413696e-01\n",
            "    1.08006162e-01  1.51734131e-01 -2.49274910e-02  7.82704202e-02\n",
            "   -1.27298022e-01 -2.13532804e-02 -8.14255202e-02  1.79364305e-01\n",
            "    2.42861195e-01 -1.43952082e-01 -1.39555338e-01 -1.52733122e-01]\n",
            "  [-1.41264938e-01 -1.13608662e-01  1.08270592e-01 -1.36413696e-01\n",
            "    1.08006162e-01  1.51734131e-01 -2.49274910e-02  7.82704202e-02\n",
            "   -1.27298022e-01 -2.13532804e-02 -8.14255202e-02  1.79364305e-01\n",
            "    2.42861195e-01 -1.43952082e-01 -1.39555338e-01 -1.52733122e-01]\n",
            "  [-1.74064268e-01  3.62870616e-03 -1.27363785e-01  6.15610794e-02\n",
            "   -3.09873773e-02 -7.65597547e-02 -3.11519252e-02 -1.88796538e-01\n",
            "    1.16888321e-01  6.44140760e-03  5.18337115e-03  2.60360697e-02\n",
            "   -6.47704024e-02  3.51244034e-02  1.35083501e-01  1.35985657e-01]\n",
            "  [ 2.06672910e-01  4.05969009e-02  5.32388610e-02 -3.16999837e-02\n",
            "   -1.40118886e-01 -9.67405826e-02  1.12811616e-02  1.87474142e-01\n",
            "    1.25178005e-01 -8.70452740e-03 -8.08296168e-02 -1.21690356e-01\n",
            "    6.46870010e-02  5.59548935e-02 -7.40330682e-02  1.83030792e-01]\n",
            "  [-1.58347106e-01 -7.57010478e-02 -1.35160384e-01  2.85885688e-01\n",
            "   -3.41412944e-02 -8.73110231e-02 -7.39005898e-02 -1.86920933e-01\n",
            "    1.39539623e-02  6.23580520e-03  1.85310133e-01  1.71071280e-01\n",
            "   -1.15487467e-01 -5.16100418e-02  1.41374039e-01  2.54305454e-02]\n",
            "  [-4.91117227e-02  1.94977667e-03 -7.83004796e-02  1.40199832e-01\n",
            "    2.90943294e-02 -1.59162127e-02  1.92015772e-03 -5.87835777e-02\n",
            "   -6.12517022e-02  1.20424597e-01 -2.77332678e-02 -7.68086952e-02\n",
            "   -7.15258073e-02 -3.59348347e-02  5.59931511e-02 -1.24149462e-01]\n",
            "  [-1.34142621e-01  5.65067838e-02  1.64667881e-03 -5.42424380e-04\n",
            "   -6.17295734e-02  1.57013226e-01  3.70292524e-02 -3.74385902e-02\n",
            "   -1.14553935e-01  4.92227736e-02 -3.16887593e-02 -7.45361350e-02\n",
            "    7.06820876e-02  5.27832238e-02 -5.51131917e-02 -2.15138675e-02]\n",
            "  [-1.18816909e-03  9.76294602e-02 -1.56921906e-01 -4.31496103e-02\n",
            "   -6.40832866e-03 -4.93256582e-02  4.81423712e-02 -1.06244330e-01\n",
            "    1.31103762e-01  8.08085606e-03 -1.89915227e-01  1.51938880e-02\n",
            "    7.54019308e-02  1.10602499e-01  7.07205413e-02  1.00973387e-01]\n",
            "  [ 1.73536596e-01  1.32082566e-02  1.90505950e-04 -7.65311259e-02\n",
            "    1.17310178e-01 -1.04391767e-01 -3.39670351e-02  1.34687822e-02\n",
            "    6.72252891e-02  8.49344020e-02  1.06572640e-01  1.24251937e-01\n",
            "   -1.20487889e-01 -1.64614391e-02 -3.12786782e-02  8.76800355e-02]\n",
            "  [ 2.76695568e-01 -4.14335002e-02 -1.52259953e-01  9.45232769e-02\n",
            "    1.64959724e-01  2.64364630e-02  1.26079014e-01  1.04413059e-01\n",
            "   -8.28100542e-02  1.63577236e-02 -1.28930124e-02  2.99598614e-01\n",
            "    1.81754842e-01 -2.85933504e-01 -1.42336191e-01 -1.56187377e-01]]\n",
            "\n",
            " [[ 2.59900531e-02 -2.09179794e-01  1.80981944e-01  7.35794089e-02\n",
            "   -1.86721316e-02 -9.49472473e-02  1.82836279e-01 -4.21970737e-02\n",
            "   -1.06068422e-02  4.31256976e-02 -1.27344883e-01  2.05955211e-01\n",
            "    2.26128558e-02 -8.60943320e-03  2.33425443e-01  1.27853243e-01]\n",
            "  [ 2.59900531e-02 -2.09179794e-01  1.80981944e-01  7.35794089e-02\n",
            "   -1.86721316e-02 -9.49472473e-02  1.82836279e-01 -4.21970737e-02\n",
            "   -1.06068422e-02  4.31256976e-02 -1.27344883e-01  2.05955211e-01\n",
            "    2.26128558e-02 -8.60943320e-03  2.33425443e-01  1.27853243e-01]\n",
            "  [-5.20993636e-02 -6.05177920e-02 -3.54037323e-02  4.38248609e-02\n",
            "    8.43708150e-02  1.90163411e-01 -1.12518437e-02  6.01162395e-02\n",
            "    4.09618135e-02 -9.31005047e-02 -2.00585517e-01 -1.77175769e-01\n",
            "    2.41841396e-02  1.47720738e-02 -3.29743780e-03 -9.10284749e-02]\n",
            "  [ 1.19342444e-02  5.87576467e-02 -1.93678118e-02  6.38718836e-02\n",
            "    1.79366745e-02  1.18718793e-01 -6.16284508e-02  5.18719585e-02\n",
            "    1.24407775e-01 -3.01800865e-02  2.63113322e-01 -7.69236217e-02\n",
            "   -1.00701408e-01 -8.26628920e-02 -1.07989411e-01  7.31435249e-02]\n",
            "  [-1.67920342e-01 -1.96201917e-02 -5.31153542e-02  1.46404923e-01\n",
            "    1.74254159e-01  2.32581904e-01 -1.19632569e-02  4.49408104e-02\n",
            "    1.00036055e-01 -2.05715462e-03 -1.76429914e-01 -8.48567204e-02\n",
            "    7.33228819e-02  2.52857175e-02 -4.87566073e-02 -1.27341457e-01]\n",
            "  [ 2.67404600e-01  1.42749645e-01  9.61268931e-03 -1.01020100e-01\n",
            "   -1.15603691e-02 -9.87225874e-02 -2.37543792e-02  9.20502508e-02\n",
            "    1.08929518e-01  5.86652947e-02  7.91718583e-02  9.43061709e-03\n",
            "    6.69349078e-02 -1.66405426e-03 -2.74766898e-02 -2.38667108e-04]\n",
            "  [ 2.02554856e-01 -2.83765552e-03 -7.55049644e-02 -1.54895899e-01\n",
            "   -1.36583144e-02 -1.23765117e-01  3.45485900e-02 -4.46588508e-02\n",
            "   -1.19534238e-02  9.35689344e-02  2.88645368e-02  9.46456202e-02\n",
            "    1.20822496e-01 -3.38788137e-02 -1.42168822e-02 -8.36775083e-02]\n",
            "  [ 1.55758812e-01 -5.46851963e-02  2.19493167e-02 -1.34408151e-02\n",
            "    1.13678682e-02  7.27656602e-02  6.16167382e-02  1.29996436e-01\n",
            "    6.10639080e-03 -1.41548387e-01 -2.00953898e-01 -1.87982594e-01\n",
            "    4.79922890e-02  9.66895721e-02 -2.48771790e-02  1.71222137e-01]\n",
            "  [ 3.52143824e-02  2.32095107e-02  1.65225708e-01  1.37225980e-01\n",
            "   -3.31969657e-02  3.00899779e-02 -3.34543861e-02  1.71592577e-02\n",
            "    7.42175125e-02  8.94323906e-02 -1.68230796e-01 -6.91376104e-02\n",
            "   -2.03951010e-02 -8.19737146e-02 -5.03500257e-02  3.13597370e-02]\n",
            "  [ 1.04303834e-01 -1.37681197e-02  1.46226893e-01  5.46786767e-02\n",
            "    3.45758936e-02 -1.92673144e-01  1.11965507e-01 -5.47734479e-03\n",
            "    1.84891704e-02  2.25385736e-01 -1.42008839e-01 -7.07518139e-03\n",
            "    7.14783876e-02  7.26866778e-02  6.11087453e-02  1.06806070e-01]]\n",
            "\n",
            " [[ 8.41288503e-02  1.22570747e-01  1.25142835e-01  1.24754652e-01\n",
            "    2.35378530e-01 -2.08957749e-03 -9.90843954e-02  1.94320229e-02\n",
            "   -4.37502218e-02 -2.10958377e-02  7.70964422e-02  3.27300110e-03\n",
            "    7.77498399e-02 -9.79451812e-03 -1.26719668e-01  1.20714130e-01]\n",
            "  [ 8.41288503e-02  1.22570747e-01  1.25142835e-01  1.24754652e-01\n",
            "    2.35378530e-01 -2.08957749e-03 -9.90843954e-02  1.94320229e-02\n",
            "   -4.37502218e-02 -2.10958377e-02  7.70964422e-02  3.27300110e-03\n",
            "    7.77498399e-02 -9.79451812e-03 -1.26719668e-01  1.20714130e-01]\n",
            "  [-1.20068955e-02 -2.00078465e-01  6.04498065e-02 -8.54711199e-02\n",
            "    2.29225407e-01 -7.20895213e-02 -8.56741592e-02 -4.66060718e-02\n",
            "    1.63048517e-01  2.49479950e-02 -9.75736759e-03 -9.65098603e-02\n",
            "    2.04030146e-02  6.05822394e-02 -4.02472286e-02 -2.38356571e-03]\n",
            "  [ 1.77821084e-01  1.70059508e-01 -1.51086271e-02 -5.63621823e-02\n",
            "   -4.43221727e-01 -1.53676595e-01 -2.83928832e-02  3.70396203e-02\n",
            "    1.88753179e-02 -8.05513990e-02  1.13928993e-01  5.98821406e-02\n",
            "   -1.76998414e-01 -1.29544899e-01  1.13960447e-02 -4.29524962e-02]\n",
            "  [-1.07648588e-01 -2.08672270e-01  4.43808850e-02 -6.97294562e-02\n",
            "    2.66255917e-01 -9.62890811e-02 -6.26110974e-02  6.41766118e-02\n",
            "    1.58381975e-01  7.73816155e-02  1.54384260e-01 -8.09452703e-02\n",
            "    2.66174857e-01  6.70798065e-02 -1.44982029e-01 -1.18838619e-02]\n",
            "  [-1.84031287e-01 -6.16090598e-02 -7.66583064e-02  1.44113348e-02\n",
            "    1.19958474e-01 -8.13982338e-02 -1.02083104e-02  7.70601958e-02\n",
            "    1.12360161e-01 -2.59594546e-02  5.82798103e-02  3.61817810e-02\n",
            "    1.12519170e-01  1.37793058e-03  1.09823474e-01 -1.96093612e-01]\n",
            "  [-1.12918451e-01 -2.97157423e-02  3.09558868e-02  1.14691856e-01\n",
            "    2.60614702e-01  9.01853587e-02  1.84127846e-02 -1.46921664e-01\n",
            "   -1.87948205e-01  2.24099773e-02  4.58309699e-02 -6.18910412e-02\n",
            "    3.69952811e-02  8.58363792e-02  1.69645119e-03  8.30664921e-02]\n",
            "  [ 8.42893035e-02 -2.66958990e-01 -3.06430933e-02 -1.20860717e-01\n",
            "   -5.92656647e-02  2.38510973e-02 -1.00601167e-01 -1.49457211e-01\n",
            "    1.32861933e-01 -9.23118096e-02 -2.15041014e-01  2.63851418e-03\n",
            "   -1.33917724e-01  1.48992981e-01  6.76261689e-02 -4.03302234e-02]\n",
            "  [ 9.28727477e-02 -8.23146340e-02 -7.79168247e-03  1.10985346e-02\n",
            "   -9.08111128e-03  1.32537725e-01  3.84562705e-02  1.30021884e-01\n",
            "    1.37638601e-01  7.96758784e-02  5.09764283e-02 -1.55841629e-01\n",
            "   -2.27549400e-01 -1.14755098e-01 -4.55802966e-03 -9.93648675e-03]\n",
            "  [ 1.37858694e-01  4.02141186e-02 -3.63198130e-02  3.13236352e-02\n",
            "   -1.60662557e-01  4.51511376e-03  9.01180554e-03  9.81991248e-02\n",
            "   -5.66477495e-02 -2.59394725e-01 -1.04880016e-01  2.76530756e-03\n",
            "    3.15093903e-02  1.62705853e-01 -2.42296712e-01 -9.48742290e-02]]] \n",
            " Shape : (3, 10, 16)\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Calculate queries, keys and values matrices\n",
        "multiHead_queries = np.matmul(embeddings, multiHead_W_q.transpose(0, 2, 1))\n",
        "multiHead_keys = np.matmul(embeddings, multiHead_W_k.transpose(0, 2, 1))\n",
        "multiHead_values = np.matmul(embeddings, multiHead_W_v.transpose(0, 2, 1))\n",
        "\n",
        "print(\"----------------------------------------\")\n",
        "print(f\"Queries:\\n {multiHead_queries} \\n Shape : {multiHead_queries.shape}\")\n",
        "print(\"----------------------------------------\")\n",
        "print(f\"Keys:\\n {multiHead_keys} \\n Shape : {multiHead_keys.shape}\")\n",
        "print(\"----------------------------------------\")\n",
        "print(f\"Values:\\n {multiHead_values} \\n Shape : {multiHead_values.shape}\")\n",
        "print(\"----------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny-cylz9_O3n"
      },
      "source": [
        "# **Cross-Attention**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju-d0OIl3lL_"
      },
      "source": [
        "**What is cross-attention, and how does it differ from self-attention?**\n",
        "\n",
        "While self-attention focuses on relationships within a single sequence (e.g., words in a sentence), cross-attention allows a model to attend to another sequence. This is essential in tasks where two different sequences interact, like in sequence-to-sequence models (e.g., translation) or multi-modal tasks (e.g., linking text and images)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqiHh1Y1_Tbu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Example sequences\n",
        "seq_1_embed = np.random.randn(5, 16)  # 5 tokens, 16-dimensional embeddings (Query source)\n",
        "seq_2_embed = np.random.randn(10, 16)  # 10 tokens, 16-dimensional embeddings (Key/Value source)\n",
        "\n",
        "# Dimensions\n",
        "embed_dim = seq_1_embed.shape[1]  # Embedding size\n",
        "attention_dim = embed_dim // 1 # Single-head attention\n",
        "\n",
        "# Initialize weight matrices\n",
        "w_q = np.random.randn(attention_dim, embed_dim)\n",
        "w_k = np.random.randn(attention_dim, embed_dim)\n",
        "w_v = np.random.randn(attention_dim, embed_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzybctsV4baf"
      },
      "source": [
        "* **The Query (Q) comes from one sequence (e.g., a target sentence).**\n",
        "* **The Key (K) and Value (V) come from another sequence (e.g., a source sentence or image features).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDuVyQmw35ED"
      },
      "outputs": [],
      "source": [
        "# Compute Query, Key, and Value\n",
        "queries = np.matmul(seq_1_embed, w_q.T)  # Q from Sequence 1\n",
        "keys = np.matmul(seq_2_embed, w_k.T)    # K from Sequence 2\n",
        "values = np.matmul(seq_2_embed, w_v.T)  # V from Sequence 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rt9AyxXw37yT"
      },
      "outputs": [],
      "source": [
        "# Compute attention scores\n",
        "attention_scores = np.matmul(queries, keys.T) / np.sqrt(attention_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCZgeLd_39TQ",
        "outputId": "4ce70b2a-63ef-49b8-9588-a442c2a5969c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention Weights Shape: (5, 10)\n",
            "Weighted Sum Shape: (5, 16)\n"
          ]
        }
      ],
      "source": [
        "# Apply softmax\n",
        "attention_weights = tf.nn.softmax(attention_scores, axis=-1)\n",
        "\n",
        "# Compute weighted sum\n",
        "weighted_sum = np.matmul(attention_weights, values)\n",
        "\n",
        "print(f\"Attention Weights Shape: {attention_weights.shape}\")\n",
        "print(f\"Weighted Sum Shape: {weighted_sum.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jKv1tCo4SUr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
